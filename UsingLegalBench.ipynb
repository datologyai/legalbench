{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a basic illustration of how to use different parts of LegalBench. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tasks import TASKS, ISSUE_TASKS\n",
    "from utils import generate_prompts\n",
    "from evaluation import EXACT_MATCH_BALANCED_ACC_TASKS, evaluate, evaluate_sara_numeric_acc, evaluate_successor_liability, evaluate_citation_open, evaluate_definition_extraction, evaluate_ssla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_org = \"Equall\"\n",
    "model_name = \"Saul-7B-Base\"\n",
    "seed = 10\n",
    "batch_size = 4\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0, # Greedy\n",
    "    seed=seed,\n",
    "    stop=[\"\\n\"],\n",
    "    max_tokens=16,\n",
    ")\n",
    "model = LLM(\n",
    "    model=f\"{hf_org}/{model_name}\",\n",
    "    trust_remote_code=True,\n",
    "    seed=seed,\n",
    "    dtype=\"float16\",\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_num_seqs=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_exact_match_tasks = copy.deepcopy(EXACT_MATCH_BALANCED_ACC_TASKS)\n",
    "\n",
    "# It needs a separtate metric\n",
    "index = available_exact_match_tasks.index(\"successor_liability\")\n",
    "del available_exact_match_tasks[index]\n",
    "\n",
    "# This task doesn't even exist\n",
    "index = available_exact_match_tasks.index(\"intra_rule_distinguishing\")\n",
    "del available_exact_match_tasks[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(available_exact_match_tasks):\n",
    "    with open(f\"tasks/{task}/base_prompt.txt\") as in_file:\n",
    "        prompt_template = in_file.read()\n",
    "\n",
    "    dataset = datasets.load_dataset(\"nguha/legalbench\", task)\n",
    "    test_df = dataset[\"test\"].to_pandas()\n",
    "    \n",
    "    prompts = generate_prompts(prompt_template=prompt_template, data_df=test_df)\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    generations = [o.outputs[0].text.strip() for o in outputs]\n",
    "    \n",
    "    accuracy = evaluate(task, generations, test_df[\"answer\"].tolist())\n",
    "    data = {\"metric_name\": \"exact_match_balanced_accuracy\", \"score\": accuracy, \"task\": task}\n",
    "    with open(f'{model_name}_evaluations.jsonl', 'a', encoding='utf-8') as f:\n",
    "        json_line = json.dumps(data, ensure_ascii=False)\n",
    "        f.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_other_tasks = [\n",
    "    'sara_numeric',\n",
    "    'successor_liability',\n",
    "    'citation_prediction_open',\n",
    "    'definition_extraction',\n",
    "    'ssla_company_defendants',\n",
    "    'ssla_individual_defendants',\n",
    "    'ssla_plaintiff',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(available_other_tasks):\n",
    "    with open(f\"tasks/{task}/base_prompt.txt\") as in_file:\n",
    "        prompt_template = in_file.read()\n",
    "\n",
    "    dataset = datasets.load_dataset(\"nguha/legalbench\", task)\n",
    "    test_df = dataset[\"test\"].to_pandas()\n",
    "    answers = test_df[\"answer\"].tolist()\n",
    "    \n",
    "    prompts = generate_prompts(prompt_template=prompt_template, data_df=test_df)\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    generations = [o.outputs[0].text.strip() for o in outputs]\n",
    "\n",
    "    if task == \"sara_numeric\":\n",
    "        accuracy = evaluate_sara_numeric_acc(generations, answers)\n",
    "    elif task == \"successor_liability\":\n",
    "        accuracy = evaluate_successor_liability(generations, answers)\n",
    "    elif task == \"citation_prediction_open\":\n",
    "        accuracy = evaluate_citation_open(generations, answers)\n",
    "    elif task == \"definition_extraction\":\n",
    "        accuracy = evaluate_definition_extraction(generations, answers)\n",
    "    elif task.startswith(\"ssla\"):\n",
    "        accuracy = evaluate_ssla(generations, answers)\n",
    "    \n",
    "    data = {\"metric_name\": \"task_specific\", \"score\": accuracy, \"task\": task}\n",
    "    with open(f'{model_name}_evaluations.jsonl', 'a', encoding='utf-8') as f:\n",
    "        json_line = json.dumps(data, ensure_ascii=False)\n",
    "        f.write(json_line + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
